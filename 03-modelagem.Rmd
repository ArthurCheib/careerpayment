---
title: "predictive_model"
author: "ACH"
date: "17 de novembro de 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
library(broom)
library(dummies)
library(dummy)
library(rsample)
library(tidyverse)
library(lubridate)
library(deflateBR)
```

Para acessar do meu PC:
```{r message=FALSE, warning=FALSE, include=FALSE}
setwd("C:/Users/arthu/OneDrive/Trabalho SEE/Datasets/pay_servidores")
load("eppggs_dataset.RData")
load("csap_dataset.RData")
```

# FIRST MODEL

First try (not enough)
```{r echo=FALSE, message=FALSE, warning=FALSE}

ml_dataset <- eppggs_dataset %>%
  filter(CSAP != "CSAP 31") %>%
  mutate(REM_POS_DEDUCOES_DEF = round(deflate(REM_POS_DEDUCOES, DATA_SALARIO, "10/2018", "ipca"), digits = 2)) %>% 
  group_by(ANO_SALARIO, CODIGO, SEXO, CONCLUSAO_GRAD) %>% 
  summarise_at(c("FERIAS", "IRRF", "CONT.PREVIDENCIRIA", "REM_POS_DEDUCOES_DEF"), mean) %>% 
  na.omit() %>% 
  select(CODIGO, SEXO,
         FERIAS, IRRF, CONT.PREVIDENCIRIA, REM_POS_DEDUCOES_DEF, ANO_SALARIO, CONCLUSAO_GRAD) %>% 
  mutate(ANO_GRADUACAO = year(CONCLUSAO_GRAD),
         ANOS_FORMADO = (ANO_SALARIO - ANO_GRADUACAO),
         SEXO_2 = case_when(SEXO == "FEMININO" ~ 0,
                          SEXO == "MASCULINO" ~ 1)) %>% 
  select(ANOS_FORMADO, CODIGO, -SEXO, SEXO_2, REM_POS_DEDUCOES_DEF) %>%
  arrange(CODIGO)

ggplot(data = ml_dataset, aes(x = ANOS_FORMADO, y = REM_POS_DEDUCOES_DEF, color = SEXO)) +
  geom_point(alpha = 1) +
  geom_smooth(method = "lm") +
  ylim(0, 20000)

b <- cor(ml_dataset$ANOS_FORMADO, ml_dataset$REM_POS_DEDUCOES_DEF)

```

Dataset for the model_lr1, 
```{r}
model_df_lr1 <- eppggs_dataset %>%
  filter(CSAP != "CSAP 31") %>%
  na.omit() %>% 
  mutate(REM_BASICA_BRUTA_DEF = round(deflate(REM_BASICA_BRUTA, DATA_SALARIO, "10/2018", "ipca"), digits = 2),
         ANOS_FORMADO = round(as.duration(CONCLUSAO_GRAD %--% DATA_SALARIO)/dyears(1), digits = 1),
         SEXO = factor(SEXO, levels = c("MASCULINO", "FEMININO")),
         CARGO = factor(CARGO, levels = c("COM CARGO", "SEM CARGO"))) %>% 
  select(SEXO, CARGO,REM_BASICA_BRUTA_DEF, ANOS_FORMADO, GRUPO_ORGAO)

model_df_lr1 <- model_df_lr1 %>% 
  cbind(as.data.frame(dummies::dummy(model_df_lr1$SEXO))) %>% 
  cbind(as.data.frame(dummies::dummy(model_df_lr1$CARGO))) %>% 
  cbind(as.data.frame(dummies::dummy(model_df_lr1$GRUPO_ORGAO)))
    
model_df_lr1 <- model_df_lr1 %>% 
  select(REM_BASICA_BRUTA_DEF, ANOS_FORMADO, `CARGO)))MASCULINO`, `CARGO)))COM CARGO`, `CARGO)))DESENVOLVIMENTO ECONOMICO`, `CARGO)))DESENVOLVIMENTO SOCIAL`, `CARGO)))EDUCACAO`, `CARGO)))FUNDACAO JOAO PINHEIRO`, `CARGO)))OUTROS`, `CARGO)))PLANEJAMENTO E GESTAO`, `CARGO)))SAUDE`, `CARGO)))SEGURANCA PUBLICA`)

real_names <- c("REM_BRUTA", "ANOS_FORMADO", "SEXO", "CARGO", "D_ECONOMICO", "D_SOCIAL", "EDUCACAO", "FJP", "OUTROS", "PLANEJAMENTO", "SAUDE", "SEG_PUBLICA")
colnames(model_df_lr1) <- real_names
```

### Spliting Train and test
To avoid introducing a bias in test using train-data, the train-test split should be performed before (most) data preparation steps. To simulate a train and test set we are going to split randomly this data set into 80% train and 20% test.

```{r echo=FALSE, message=FALSE, warning=FALSE}
#Creating training and test datasets:
set.seed(123)
split  <- initial_split(model_df_lr1, prop = 0.8)
train_dataset <- training(split)
test_dataset <- testing(split)

g1 <- ggplot(data = train_dataset, aes(x = ANOS_FORMADO, y = REM_BRUTA)) +
  geom_col()

g2 <- ggplot(data = test_dataset, aes(x = ANOS_FORMADO, y = REM_BRUTA)) +
  geom_col()

gridExtra::grid.arrange(g1, g2, ncol = 2)
```

Creating the data frame for comparation
```{r}
modelMetrics <- data.frame( "Model" = character(), "adjRsq" = integer(),  "AIC"= integer(), "BIC"= integer(), "Mean Prediction Error"= integer(), "Standard Error"= integer(), stringsAsFactors=FALSE)
```

Running the first model with variables income explained by years
```{r echo=FALSE, message=FALSE, warning=FALSE}
# Beginning of the modeling number ONE:
model_lr1 <- lm(formula = REM_BRUTA ~ ANOS_FORMADO, data = train_dataset)

summary(model_lr1)

income_predicted <- predict(model_lr1, test_dataset)

preds <- data.frame(cbind(valores_reais = test_dataset$REM_BRUTA, valores_preditos = income_predicted))
correlation_accuracy <- cor(preds)
head(preds, n = 10)

# Inserting erros parameters for this model with these variables:
aic <- round(AIC(model_lr1), digits = 4)
bic <- round(BIC(model_lr1), digits = 4)


pred_model_lr1 <- predict(model_lr1, newdata = test_dataset) # validation predictions
meanPred <- round(mean((test_dataset$REM_BRUTA - pred_model_lr1)^2), digits = 4) # mean prediction error
stdError <- round(sd((test_dataset$REM_BRUTA - pred_model_lr1)^2)/length(test_dataset$REM_BRUTA), digits = 4)

modelMetrics[nrow(modelMetrics) + 1, ] <- c( "model_lr1", 0.7025, aic, bic, meanPred, stdError)
```

Running the first model with variables income explained by years + job position
```{r echo=FALSE, message=FALSE, warning=FALSE}
# Beginning of the modeling number TWO:
model_lr2 <- lm(formula = REM_BRUTA ~ ANOS_FORMADO + CARGO, data = train_dataset)

summary(model_lr2)

income_predicted <- predict(model_lr2, test_dataset)

preds <- data.frame(cbind(valores_reais = test_dataset$REM_BRUTA, valores_preditos = income_predicted))
correlation_accuracy <- cor(preds)
head(preds, n = 10)

# Inserting erros parameters for this model with these variables:
aic <- round(AIC(model_lr2), digits = 4)
bic <- round(BIC(model_lr2), digits = 4)


pred_model_lr2 <- predict(model_lr2, newdata = test_dataset) # validation predictions
meanPred <- round(mean((test_dataset$REM_BRUTA - pred_model_lr2)^2), digits = 4) # mean prediction error
stdError <- round(sd((test_dataset$REM_BRUTA - pred_model_lr2)^2)/length(test_dataset$REM_BRUTA), digits = 4)

modelMetrics[nrow(modelMetrics) + 1, ] <- c( "model_lr2", 0.7535, aic, bic, meanPred, stdError)
```

Running the first model with variables income explained by years + job position + sex
```{r echo=FALSE, message=FALSE, warning=FALSE}
# Beginning of the modeling number TWO:
model_lr3 <- lm(formula = REM_BRUTA ~ ANOS_FORMADO + CARGO + SEXO, data = train_dataset)

summary(model_lr3)

income_predicted <- predict(model_lr3, test_dataset)

preds <- data.frame(cbind(valores_reais = test_dataset$REM_BRUTA, valores_preditos = income_predicted))
correlation_accuracy <- cor(preds)
head(preds, n = 10)

# Inserting erros parameters for this model with these variables:
aic <- round(AIC(model_lr3), digits = 4)
bic <- round(BIC(model_lr3), digits = 4)


pred_model_lr3 <- predict(model_lr3, newdata = test_dataset) # validation predictions
meanPred <- round(mean((test_dataset$REM_BRUTA - pred_model_lr3)^2), digits = 4) # mean prediction error
stdError <- round(sd((test_dataset$REM_BRUTA - pred_model_lr3)^2)/length(test_dataset$REM_BRUTA), digits = 4)

modelMetrics[nrow(modelMetrics) + 1, ] <- c( "model_lr3", 0.7539, aic, bic, meanPred, stdError)
```

Checking the results:
```{r}
modelMetrics
```


Detecting influential points of the data used in the model building:
If I have in my data influential points that might skew my model, sometimes unnecessarily. Think of a mistake on the data entry and instead of writing “2.3” the value was “23”. The most common kind of influential point are the outliers, which are data points where the observed response does not appear to follow the pattern established by the rest of the data.

```{r}

plot(cooks.distance(modelo), pch = 16, col = "blue") #Plot the Cooks Distances.
```

### The p Value: Checking for statistical significance
The summary statistics above tells us a number of things. One of them is the model p-Value (bottom last line) and the p-Value of individual predictor variables (extreme right column under ‘Coefficients’). The p-Values are very important because, We can consider a linear model to be statistically significant only when both these p-Values are less that the pre-determined statistical significance level, which is ideally 0.05. This is visually interpreted by the significance stars at the end of the row. The more the stars beside the variable’s p-Value, the more significant the variable.

#### Null and alternate hypothesis
When there is a p-value, there is a hull and alternative hypothesis associated with it. In Linear Regression, the Null Hypothesis is that the coefficients associated with the variables is equal to zero. The alternate hypothesis is that the coefficients are not equal to zero (i.e. there exists a relationship between the independent variable in question and the dependent variable).

### R-Squared and Adj R-Squared
What R-Squared tells us is the proportion of variation in the dependent (response) variable that has been explained by this model.

Now thats about R-Squared. What about adjusted R-Squared? As you add more X variables to your model, the R-Squared value of the new bigger model will always be greater than that of the smaller subset. This is because, since all the variables in the original model is also present, their contribution to explain the dependent variable will be present in the super-set as well, therefore, whatever new variable we add can only add (if not significantly) to the variation that was already explained. It is here, the adjusted R-Squared value comes to help. Adj R-Squared penalizes total value for the number of terms (read predictors) in your model. Therefore when comparing nested models, it is a good practice to look at adj-R-squared value over R-squared.